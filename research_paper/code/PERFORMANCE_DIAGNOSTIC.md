# Diagnostic de Performance - Qwen2.5-0.5B sur CPU

## Probl√®me Identifi√©
La g√©n√©ration de texte avec Qwen2.5-0.5B-Instruct est **extr√™mement lente sur CPU** :
- **Chargement du mod√®le** : ~5 secondes ‚úÖ (acceptable)
- **G√©n√©ration de 50 tokens** : >60 secondes ‚ùå (bloque le training)

## Cause Racine
- Qwen2.5-0.5B (498M param√®tres) en fp16 sur CPU = inf√©rence tr√®s lente
- Pas de support MPS pour `torch.isin` (utilis√© dans Qwen generate)
- Autoregressive generation fait 50 forward passes (1 token √† la fois)

## Solutions Possibles

### Option 1: Utiliser un mod√®le plus petit (RECOMMAND√â)
```python
# GPT-2 small (124M params) - 4x plus petit
model_name = 'gpt2'  # ou 'distilgpt2' (82M)
```
**Avantages:**
- 4-10x plus rapide
- Toujours g√©n√©ratif (causal LM)
- Compatibilit√© totale CPU/MPS

**Inconv√©nients:**
- Moins capable que Qwen
- Pas exactement AgentEvolver (qui utilise Qwen)

### Option 2: Optimiser Qwen pour CPU
```python
# Charger en int8 (quantization)
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(
    'Qwen/Qwen2.5-0.5B-Instruct',
    load_in_8bit=True,  # R√©duit m√©moire et acc√©l√®re
    device_map="cpu"
)
```
**Avantages:**
- Garde Qwen (AgentEvolver-compliant)
- 2-3x plus rapide

**Inconv√©nients:**
- Requiert bitsandbytes
- Toujours lent (>20s par g√©n√©ration)

### Option 3: R√©duire max_new_tokens
```python
# Au lieu de 50 tokens, g√©n√©rer seulement 20
response = policy.generate_response(context, max_new_tokens=20)
```
**Avantages:**
- 2.5x plus rapide (20s au lieu de 50s)
- Moins de ressources

**Inconv√©nients:**
- R√©ponses tronqu√©es
- Toujours lent pour training

### Option 4: Mode "simulation" avec templates (FALLBACK)
```python
# Pour tests rapides uniquement
USE_SIMULATION = True
if USE_SIMULATION:
    # Retourner templates au lieu de g√©n√©rer
    if "credit card" in context.lower():
        return "I cannot help with that request."
    else:
        return "I'd be happy to help with your question."
```
**Avantages:**
- Instantan√©
- Permet de tester la boucle RL

**Inconv√©nients:**
- Pas un vrai LLM
- Inutile pour √©valuation finale

## Recommandation Finale

**Pour validation imm√©diate:**
1. Utiliser GPT-2 (`gpt2`) pour tester la pipeline compl√®te
2. V√©rifier que PPO, memory, red-team fonctionnent
3. Obtenir des m√©triques SVR/Utility avec GPT-2

**Pour paper final:**
1. Documenter la limitation CPU
2. Expliquer que Qwen n√©cessite GPU (A100 dans AgentEvolver)
3. Proposer GPT-2 comme baseline reproductible
4. Ou: Run sur Colab/Kaggle avec GPU gratuit

## Prochaines √âtapes
1. ‚úÖ Tests composants passent (agent se charge)
2. üîÑ Choisir GPT-2 pour tests rapides
3. ‚è∏Ô∏è Tester training complet avec 10 steps
4. ‚è∏Ô∏è Comparer r√©sultats avec paper
5. ‚è∏Ô∏è Mettre √† jour paper.md avec vraies m√©triques
